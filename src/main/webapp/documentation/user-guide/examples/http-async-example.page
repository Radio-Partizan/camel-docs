---
title: HTTP Async Example
page_version: 5
page_creator: davsclaus
page_modifier: davsclaus
--- pipeline:conf
h2. HTTP Async Example

This example shows how to use the new feature in Camel 2.1 which is support for non blocking asynchronous producers by [ToAsync].

Currently [camel-jetty|Jetty] implements this to the fullest as its {{JettyHttpProducer}} supports non blocking request/reply natively in Jetty.

This example shows a client and a server in action. The client sends 100 messages to the server over [HTTP|Jetty] which the server processes and returns a reply.

The client is working using a single threaded to route the messages to the point where they are send to the HTTP server. As we use non blocking asynchronous [Request reply] this single thread will terminate its current task and be ready immediately to route the next message. This allows us to have higher throughput as the single thread can go as fast as it can, it does not have to wait for the HTTP server to reply (i.e. its not blocking).

You can see the difference if you change the {{async=true}} option to {{async=false}} in the {{src/main/resources/META-INF/spring/camel-client.xml}} file.

h3. Running

You will need to compile this example first:
{code}
  mvn compile
{code}

The example should run if you type:
{code}
  mvn exec:java -PCamelServer

  mvn exec:java -PCamelClient
{code}

To stop the server hit ctrl + c

h3. Sample output
When the client is running it outputs all requests and responses on the screen.

As the client is single threaded it will send the messages in order, e.g. from 0 to 99.
{code}
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 0]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 1]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 2]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 3]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 4]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 5]
[          org.apache.camel.example.client.CamelClient.main()] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 6]
...
{code}

As the HTTP server is simulating some time to process each message its replies will likely come after all the client have send all 100 messages. When they arrive they come back out of order
{code}
[  Camel thread 2: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 7]
[  Camel thread 3: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 27]
[  Camel thread 1: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 2]
[  Camel thread 6: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 5]
[  Camel thread 4: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 3]
[  Camel thread 7: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 28]
[  Camel thread 5: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 24]
[  Camel thread 8: ToAsync[jetty://http://0.0.0.0:9123/myapp]] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 9]
{code}

And as you can see they are being handled by different threads, as we have configured using a {{poolSize=10}} option.

h3. Running synchronous
If we on the other hand change to synchronous mode, that means we will use the single thread for the entire routing and it will be blocked while waiting for the reply from the HTTP server. To see this in action change the {{async="true"}} to {{async="false"}}. 

The output is then as expected a request, reply and so forth. And of course the throughput is much lower as we are only handle a single message at the time and blocked while waiting for the HTTP server reply.

{code}
[                                                        main] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 4]
[                                                        main] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 4]
[                                                        main] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 5]
[                                                        main] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 5]
[                                                        main] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 6]
[                                                        main] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 6]
[                                                        main] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 7]
[                                                        main] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 7]
[                                                        main] +++ request +++      INFO  Exchange[BodyType:String, Body:Message 8]
[                                                        main] +++  reply  +++      INFO  Exchange[BodyType:String, Body:Bye Message 8]
{code}


h3. See Also
- [Examples]
- [ToAsync]
- [Jetty]